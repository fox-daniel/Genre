{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This nb builds a classifier to predict gender from genre using Gradient Boosted Trees. Transformations to sparse matrix reps, tfidf, and truncated svd appear to be very helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Feature Importance: ESL 10.13\n",
    "- [ ] Create a further dimension reduction to 2D using most important features\n",
    "- [ ] Create a further dimension reduction to 2D by recalculating truncated SVD\n",
    "- [ ] Implement subset selection of features: https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import genre_data_loader, genre_upperbound\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns; sns.set()\n",
    "\n",
    "# import re\n",
    "\n",
    "# import os\n",
    "# from gensim import corpora\n",
    "# from gensim.corpora import MmCorpus\n",
    "# from gensim.models import TfidfModel, LsiModel\n",
    "# from gensim.matutils import corpus2dense\n",
    "\n",
    "# import json\n",
    "\n",
    "seed = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get currrent date for latest version of data set\n",
    "%store -r now\n",
    "\n",
    "X_path_train = '/Users/Daniel/Code/Genre/data/genre_lists/data_ready_for_model/wiki-kaggle_X_train_{}.csv'.format(now)\n",
    "y_path_train = '/Users/Daniel/Code/Genre/data/genre_lists/data_ready_for_model/wiki-kaggle_y_train_{}.csv'.format(now)\n",
    "X_path_test = '/Users/Daniel/Code/Genre/data/genre_lists/data_ready_for_model/wiki-kaggle_X_test_{}.csv'.format(now)\n",
    "y_path_test = '/Users/Daniel/Code/Genre/data/genre_lists/data_ready_for_model/wiki-kaggle_y_test_{}.csv'.format(now)\n",
    "\n",
    "# call data loader script for training data\n",
    "genre_data = genre_data_loader.LoadGenreData(now, X_path_train = X_path_train, y_path_train = y_path_train,\n",
    "                                                  X_path_test = X_path_test, y_path_test = y_path_test)\n",
    "# call it for train data\n",
    "genre_data_train = genre_data_loader.LoadGenreData(now, X_path_train = X_path_train, y_path_train = y_path_train)\n",
    "# call it for test data\n",
    "genre_data_test = genre_data_loader.LoadGenreData(now, X_path_train = X_path_test, y_path_train = y_path_test)\n",
    "\n",
    "# load data with genre sets\n",
    "data_train = genre_data_train.as_strings()\n",
    "data_train = genre_data_train.as_lists()\n",
    "data_train = genre_data_train.as_sets()\n",
    "data_balanced = genre_data_train.get_balanced_sample()\n",
    "\n",
    "data_test = genre_data_test.as_strings()\n",
    "\n",
    "data = genre_data.as_strings()\n",
    "data = genre_data.as_sets()\n",
    "\n",
    "# create list of all genres\n",
    "list_of_genres = genre_data.get_list_of_genres()\n",
    "\n",
    "# Create a dictionary of {genre : genre_id}\n",
    "dict_gid = genre_data.get_dict_genre_to_id()\n",
    "dict_idg = genre_data.get_dict_id_to_genre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12376, 5), (3094, 3), (15470, 4))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape, data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a sparse data structure encoding of the genre labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_sparse(data):\n",
    "\n",
    "    vec = CountVectorizer(vocabulary = dict_gid) # uses scipy.sparse.csr_matrix representation\n",
    "    # create sparse matrix of training features NOTE: cannot use .get_sparse_matrix() method of genre_data_loader:\n",
    "    # that will use a dict_gid built from the list of genres only from training data \n",
    "    # full training set\n",
    "    X = vec.fit_transform(data.genre_string)\n",
    "    # balanced sample\n",
    "    X = vec.fit_transform(data.genre_string)\n",
    "\n",
    "\n",
    "    # Encode labels:\n",
    "    # le = preprocessing.LabelEncoder()\n",
    "    # le.fit(['male', 'female'])\n",
    "    # #le.classes_\n",
    "    # y = le.transform(data_train.gender.values)\n",
    "    # # le.transform(['female'])\n",
    "    # # le.inverse_transform([1,0,1])\n",
    "\n",
    "    # Encode Labels with UDF so can control encoding:\n",
    "    def encode_targets(row):\n",
    "        if row.gender == 'female':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    y = data.apply(encode_targets, axis = 1)\n",
    "    y = y.values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_bal, y_bal = transform_to_sparse(data_balanced)\n",
    "# X_train, y_train = transform_to_sparse(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce sparse -> tfidf -> truncated svd -> model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_svd(data):\n",
    "\n",
    "    vec = CountVectorizer(vocabulary = dict_gid) # uses scipy.sparse.csr_matrix representation\n",
    "    # create sparse matrix of training features NOTE: cannot use .get_sparse_matrix() method of genre_data_loader:\n",
    "    # that will use a dict_gid built from the list of genres only from training data \n",
    "    \n",
    "    # fit and transform\n",
    "    X = vec.fit_transform(data.genre_string)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X = tfidf_transformer.fit_transform(X)\n",
    "    svd_transformer = TruncatedSVD(n_components = 250)\n",
    "    svd_transformer.fit(X)\n",
    "    X = svd_transformer.transform(X)\n",
    "    \n",
    "    # Encode Labels with UDF so can control encoding:\n",
    "    def encode_targets(row):\n",
    "        if row.gender == 'female':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    y = data.apply(encode_targets, axis = 1)\n",
    "    y = y.values\n",
    "    \n",
    "    # transformers\n",
    "    bow_transform = vec.transform\n",
    "    tfidf_transform = tfidf_transformer.transform\n",
    "    svd_transform = svd_transformer.transform\n",
    "    \n",
    "    return X, y, bow_transform, tfidf_transform, svd_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, bow_transform, tfidf_transform, svd_transform = transform_to_svd(data_train)\n",
    "# X_bal, y_bal, bow_transform, tfidf_transform, svd_transform = transform_to_svd(data_balanced)\n",
    "def full_transform(document):\n",
    "    return svd_transform(tfidf_transform(bow_transform(document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_transform(data_balanced.genre_string.iloc[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_transform(data_balanced.genre_string.iloc[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test alignment of data, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From genre_data_loader.ipynb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decode_sparse_list(X_sparse, row_number):\n",
    "#     zeros, cols = X_sparse[row_number].nonzero()\n",
    "#     cols_labels = [dict_idg[ind] for ind in cols]\n",
    "#     cols_labels.sort()\n",
    "#     return cols_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check alignment of data_train and (X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = np.random.randint(data_balanced.shape[0])\n",
    "# sorted(decode_sparse_list(X, n)), sorted(data_balanced.genre_list.iloc[n])\n",
    "# n = np.random.randint(data_balanced.shape[0])\n",
    "# y[n], data_balanced.gender.iloc[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Convert inputs to a numpy array and then create a scaler class to normalize the feature values that can be applied to training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #scaler = preprocessing.StandardScaler(with_mean = False).fit(X_sparse) # need with_mean = False for sparse data\n",
    "# # transformer\n",
    "# transformer = preprocessing.MaxAbsScaler(copy = False).fit(X_train)\n",
    "# transformer.scale_.max(), transformer.max_abs_.max()\n",
    "# Apply the scaler to the training data:\n",
    "# X_scaled = transformer.transform(X_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate the model: GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators = 1000, \n",
    "                                   learning_rate = .5,\n",
    "                                   subsample = .8,\n",
    "                                   loss = 'deviance',\n",
    "                                   max_depth = 4,\n",
    "                                   min_samples_split = 2,\n",
    "                                   random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive train and score insample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_in_sample(model, data, X, y):\n",
    "    mal = data[data.gender == 'male'].shape[0]\n",
    "    fem = data[data.gender == 'female'].shape[0]\n",
    "    p_mal = mal/(mal+fem)\n",
    "    model.fit(X,y)\n",
    "    score = model.score(X, y)\n",
    "    print(\"_______________________________\")\n",
    "    print('Basic Report on In Sample Score')\n",
    "    print(f'The baseline accuracy by always classifying male on the training set is {round(p_mal,2)}.')\n",
    "    print(f'Accuracy on the training set is {round(score,3)}.')\n",
    "    uppers, err = genre_upperbound.UpperBound(data)\n",
    "    print(f'Upper bound to accuracy on the training set is {round(1-err,3)}')\n",
    "    print(f'Accuracy is {round(100*(score)/(1-err),1)}% of the upperbound.')\n",
    "    print(f'Accuracy is {round(100*(score-p_mal)/(1-err-p_mal),1)}% improvement over the baseline of {100*round(p_mal,2)}%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________\n",
      "Basic Report on In Sample Score\n",
      "The baseline accuracy by always classifying male on the training set is 0.69.\n",
      "Accuracy on the training set is 0.844.\n",
      "Upper bound to accuracy on the training set is 0.866\n",
      "Accuracy is 97.5% of the upperbound.\n",
      "Accuracy is 87.6% improvement over the baseline of 69.0%.\n"
     ]
    }
   ],
   "source": [
    "# report_in_sample(model, data_balanced, X_bal, y_bal)\n",
    "report_in_sample(model, data_train, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def embedding(sparse_features, targets):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search with CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split into train and validate for the grid search\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .3)\n",
    "\n",
    "\n",
    "# tuning_parameters = [{'n_estimators': np.linspace(200, 2000, 2, dtype = 'int32').tolist(), \n",
    "#                       'learning_rate': np.linspace(.1, 2, 2).tolist()}]\n",
    "\n",
    "# clf = GridSearchCV(GradientBoostingClassifier(), tuning_parameters) \n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # # scores on training folds\n",
    "# # means = clf.cv_results_['mean_test_score']\n",
    "# # std = clf.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = y_val, clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(model, x_data, y_data, n_splits = 2):\n",
    "    \"\"\"This function takes a model, features, targets, and number of folds and returns\n",
    "    accuracy scores and the validation set index for each fold\n",
    "    Input:\n",
    "        model - a model that has .fit, .score, .predict methods\n",
    "        X - sparse matrix representing features: genre labels\n",
    "        y - list of genders\n",
    "        \n",
    "    Output:\n",
    "        cvscores - list of cvscores, \n",
    "        cms - list of confusion matrices, \n",
    "        vals - list of validation set indices. \n",
    "    It also prints basic stats.\n",
    "    \"\"\"\n",
    "\n",
    "    kf = KFold(n_splits = n_splits, shuffle = True, random_state = seed)\n",
    "\n",
    "    cvscores = []\n",
    "    #cms = []\n",
    "    vals = []\n",
    "    \n",
    "    for train, val in kf.split(x_data,y_data):\n",
    "        X_train = x_data[train]\n",
    "        y_train = y_data[train]\n",
    "\n",
    "        model.fit(X_train, y_train);\n",
    "\n",
    "        X_val = x_data[val]\n",
    "        y_val = y_data[val]\n",
    "\n",
    "        score = model.score(X_val, y_val)\n",
    "        cvscores.append(round(score,3))\n",
    "\n",
    "        # compute confusion matrices and store them in a list\n",
    "        #y_pred = model.predict(X_val)\n",
    "        #cms.append(confusion_matrix(y_val, y_pred))\n",
    "        \n",
    "        vals.append(val)\n",
    "\n",
    "        # calculate percent male\n",
    "        number_fem = y_data.sum()\n",
    "        number_mal = y_data.shape[0]-number_fem\n",
    "        percent_mal = 100*round(number_mal/(number_fem+number_mal),1)\n",
    "        \n",
    "    print(f'Given that {percent_mal}% of the artists are male, a random guess would have an accuracy of {percent_mal}%.')\n",
    "    print(f'For the model, the mean accuracy is {100*np.mean(cvscores):.2f}% and 100*STD is {100*np.std(cvscores):.2f}%')\n",
    "    print(f'This is a {100*(100*np.mean(cvscores)-percent_mal)/percent_mal:.2f}% improvement over a random guess.')\n",
    "        \n",
    "    return cvscores, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyper parameters\n",
    "n_estimators = 3000\n",
    "learning_rate = .05\n",
    "subsample = .8\n",
    "loss = 'deviance'\n",
    "max_depth = 6\n",
    "min_samples_split = 2\n",
    "random_state = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators = n_estimators, \n",
    "                                   learning_rate = learning_rate,\n",
    "                                   subsample = subsample,\n",
    "                                   loss = loss,\n",
    "                                   max_depth = max_depth,\n",
    "                                   min_samples_split = min_samples_split,\n",
    "                                   random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given that 70.0% of the artists are male, a random guess would have an accuracy of 70.0%.\n",
      "For the model, the mean accuracy is 72.86% and 100*STD is 0.64%\n",
      "This is a 4.09% improvement over a random guess.\n"
     ]
    }
   ],
   "source": [
    "# cvscores, vals = train_validate(model, X_bal, y_bal, 5)\n",
    "cvscores, vals = train_validate(model, X_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.725, 0.73, 0.727, 0.74, 0.721]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV SCORES: $[0.725, 0.73, 0.727, 0.74, 0.721]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate upper bounds on accuracy for each validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_bounds(vals, data):\n",
    "    \"\"\"Create a list of the upper bounds on accuracy for each validation set.\"\"\"\n",
    "    uppers = []\n",
    "    for val in vals:\n",
    "        data_val = data.iloc[val] \n",
    "        upper, error = genre_upperbound.UpperBound(data_val)\n",
    "        uppers.append(round(1-error,3))\n",
    "    return uppers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "uppers = upper_bounds(vals, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9, 0.899, 0.897, 0.895, 0.901]\n",
      "[0.725, 0.73, 0.727, 0.74, 0.721]\n",
      "[0.175 0.169 0.17  0.155 0.18 ]\n"
     ]
    }
   ],
   "source": [
    "print(uppers)\n",
    "print(cvscores)\n",
    "print(np.array(uppers)-np.array(cvscores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The upper bound on accuracy for the validation sets is ~90% and the model achieves ~73%.\n",
    "\n",
    "### Try using a neural embedding layer before applying the GBT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check alignment of genders\n",
    "# n = np.random.randint(y_val0.shape[0])\n",
    "# print('y_val')\n",
    "# print(y_val0[n])\n",
    "# print('data_val')\n",
    "# print(data_val0.gender.iloc[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances  = model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_import = pd.DataFrame({'importance':importances})\n",
    "feat_import.index.name = 'feature'\n",
    "feat_import.reset_index()\n",
    "feat_import.sort_values(['importance'], ascending = False, inplace = True)\n",
    "feat_import.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_transformer.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

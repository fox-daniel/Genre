{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "\n",
    "X, y are of the training set; the test set is not used yet, but saved till all modeling work is complete for a final evaluation; thus X_train, X_val refer to subsets of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This nb builds a classifier to predict gender from genre using a random forest model.\n",
    "\n",
    "We look at the following encoding/embeddings:\n",
    "- [ ] BOW\n",
    "- [ ] TFIDF\n",
    "- [ ] LSI\n",
    "- [ ] LDA\n",
    "- [ ] Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import genre_data_loader, genre_upperbound\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns; sns.set()\n",
    "\n",
    "# import re\n",
    "\n",
    "# import os\n",
    "# from gensim import corpora\n",
    "# from gensim.corpora import MmCorpus\n",
    "# from gensim.models import TfidfModel, LsiModel\n",
    "# from gensim.matutils import corpus2dense\n",
    "\n",
    "# import json\n",
    "\n",
    "seed = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get currrent date for latest version of data set\n",
    "%store -r now\n",
    "\n",
    "X_path_train = '/Users/Daniel/Code/Genre/data/genre_lists/data_ready_for_model/wiki-kaggle_X_train_{}.csv'.format(now)\n",
    "y_path_train = '/Users/Daniel/Code/Genre/data/genre_lists/data_ready_for_model/wiki-kaggle_y_train_{}.csv'.format(now)\n",
    "X_path_test = '/Users/Daniel/Code/Genre/data/genre_lists/data_ready_for_model/wiki-kaggle_X_test_{}.csv'.format(now)\n",
    "y_path_test = '/Users/Daniel/Code/Genre/data/genre_lists/data_ready_for_model/wiki-kaggle_y_test_{}.csv'.format(now)\n",
    "\n",
    "# call data loader script for training data\n",
    "genre_data = genre_data_loader.LoadGenreData(now, X_path_train = X_path_train, y_path_train = y_path_train,\n",
    "                                                  X_path_test = X_path_test, y_path_test = y_path_test)\n",
    "# call it for train data\n",
    "genre_data_train = genre_data_loader.LoadGenreData(now, X_path_train = X_path_train, y_path_train = y_path_train)\n",
    "# call it for test data\n",
    "genre_data_test = genre_data_loader.LoadGenreData(now, X_path_train = X_path_test, y_path_train = y_path_test)\n",
    "\n",
    "# load data with genre sets\n",
    "data_train = genre_data_train.as_strings()\n",
    "data_train = genre_data_train.as_lists()\n",
    "data_train = genre_data_train.as_sets()\n",
    "data_balanced = genre_data_train.get_balanced_sample()\n",
    "\n",
    "data_test = genre_data_test.as_strings()\n",
    "\n",
    "data = genre_data.as_strings()\n",
    "data = genre_data.as_sets()\n",
    "\n",
    "# create list of all genres\n",
    "list_of_genres = genre_data.get_list_of_genres()\n",
    "\n",
    "# Create a dictionary of {genre : genre_id}\n",
    "dict_gid = genre_data.get_dict_genre_to_id()\n",
    "dict_idg = genre_data.get_dict_id_to_genre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12376, 5), (3094, 3), (15470, 4))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape, data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a sparse data structure encoding of the genre labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(vocabulary = dict_gid) # uses scipy.sparse.csr_matrix representation\n",
    "# create sparse matrix of training features NOTE: cannot use .get_sparse_matrix() method of genre_data_loader:\n",
    "# that will use a dict_gid built from the list of genres only from training data \n",
    "# full training set\n",
    "X = vec.fit_transform(data_train.genre_string)\n",
    "# balanced sample\n",
    "X = vec.fit_transform(data_balanced.genre_string)\n",
    "\n",
    "\n",
    "# Encode labels:\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# le.fit(['male', 'female'])\n",
    "# #le.classes_\n",
    "# y = le.transform(data_train.gender.values)\n",
    "# # le.transform(['female'])\n",
    "# # le.inverse_transform([1,0,1])\n",
    "\n",
    "# Encode Labels with UDF so can control encoding:\n",
    "def encode_targets(row):\n",
    "    if row.gender == 'female':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "y = data_balanced.apply(encode_targets, axis = 1)\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test alignment of data, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From genre_data_loader.ipynb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sparse_list(X_sparse, row_number):\n",
    "    zeros, cols = X_sparse[row_number].nonzero()\n",
    "    cols_labels = [dict_idg[ind] for ind in cols]\n",
    "    cols_labels.sort()\n",
    "    return cols_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check alignment of data_train and (X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'female')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.random.randint(data_balanced.shape[0])\n",
    "sorted(decode_sparse_list(X, n)), sorted(data_balanced.genre_list.iloc[n])\n",
    "n = np.random.randint(data_balanced.shape[0])\n",
    "y[n], data_balanced.gender.iloc[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Convert inputs to a numpy array and then create a scaler class to normalize the feature values that can be applied to training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #scaler = preprocessing.StandardScaler(with_mean = False).fit(X_sparse) # need with_mean = False for sparse data\n",
    "# # transformer\n",
    "# transformer = preprocessing.MaxAbsScaler(copy = False).fit(X_train)\n",
    "# transformer.scale_.max(), transformer.max_abs_.max()\n",
    "# Apply the scaler to the training data:\n",
    "# X_scaled = transformer.transform(X_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate the model: GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators = 500, learning_rate = 1.5, loss = 'exponential', random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive train and score insample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=1.5, loss='exponential', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=23, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline accuracy by always classifying male on the training set is 0.5.\n",
      "Accuracy on the training set is 0.8218092019755654.\n",
      "Upper bound to accuracy on the training set is 0.853782\n",
      "Accuracy is 96.0% of the upperbound.\n",
      "Accuracy is 91.0% improvement over the baseline of 69%.\n"
     ]
    }
   ],
   "source": [
    "def report_in_sample(data, X, y):\n",
    "    mal = data[data.gender == 'male'].shape[0]\n",
    "    fem = data[data.gender == 'female'].shape[0]\n",
    "    p_mal = mal/(mal+fem)\n",
    "    score = model.score(X, y)\n",
    "    print(f'The baseline accuracy by always classifying male on the training set is {round(p_mal,2)}.')\n",
    "    print(f'Accuracy on the training set is {score}.')\n",
    "    uppers, err = genre_upperbound.UpperBound(data)\n",
    "    print(f'Upper bound to accuracy on the training set is {1-err}')\n",
    "    print(f'Accuracy is {100*round((score)/(1-err),2)}% of the upperbound.')\n",
    "    print(f'Accuracy is {100*round((score-p_mal)/(1-err-p_mal),2)}% improvement over the baseline of 69%.')\n",
    "    \n",
    "report_in_sample(data_balanced, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_in_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search with CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split into train and validate for the grid search\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .3)\n",
    "\n",
    "\n",
    "# tuning_parameters = [{'n_estimators': np.linspace(200, 2000, 2, dtype = 'int32').tolist(), \n",
    "#                       'learning_rate': np.linspace(.1, 2, 2).tolist()}]\n",
    "\n",
    "# clf = GridSearchCV(GradientBoostingClassifier(), tuning_parameters) \n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # # scores on training folds\n",
    "# # means = clf.cv_results_['mean_test_score']\n",
    "# # std = clf.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = y_val, clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones(6)\n",
    "x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(model, x_data, y_data, n_splits = 2):\n",
    "    \"\"\"This function takes a model, features, targets, and number of folds and returns\n",
    "    accuracy scores and the validation set index for each fold\n",
    "    Input:\n",
    "        model - a model that has .fit, .score, .predict methods\n",
    "        X - sparse matrix representing features: genre labels\n",
    "        y - list of genders\n",
    "        \n",
    "    Output:\n",
    "        cvscores - list of cvscores, \n",
    "        cms - list of confusion matrices, \n",
    "        vals - list of validation set indices. \n",
    "    It also prints basic stats.\n",
    "    \"\"\"\n",
    "\n",
    "    kf = KFold(n_splits = n_splits, shuffle = True, random_state = seed)\n",
    "\n",
    "    cvscores = []\n",
    "    #cms = []\n",
    "    vals = []\n",
    "    \n",
    "    for train, val in kf.split(x_data,y_data):\n",
    "        X_train = x_data[train]\n",
    "        y_train = y_data[train]\n",
    "\n",
    "        model.fit(X_train, y_train);\n",
    "\n",
    "        X_val = x_data[val]\n",
    "        y_val = y_data[val]\n",
    "\n",
    "        score = model.score(X_val, y_val)\n",
    "        cvscores.append(round(score,3))\n",
    "\n",
    "        # compute confusion matrices and store them in a list\n",
    "        #y_pred = model.predict(X_val)\n",
    "        #cms.append(confusion_matrix(y_val, y_pred))\n",
    "        \n",
    "        vals.append(val)\n",
    "\n",
    "        # calculate percent male\n",
    "        number_fem = y_data.sum()\n",
    "        number_mal = y_data.shape[0]-number_fem\n",
    "        percent_mal = 100*round(number_mal/(number_fem+number_mal),1)\n",
    "        \n",
    "    print(f'Given that {percent_mal}% of the artists are male, a random guess would have an accuracy of {percent_mal}%.')\n",
    "    print(f'For the model, the mean accuracy is {100*np.mean(cvscores):.2f}% and 100*STD is {100*np.std(cvscores):.2f}%')\n",
    "    print(f'This is a {100*(100*np.mean(cvscores)-percent_mal)/percent_mal:.2f}% improvement over a random guess.')\n",
    "        \n",
    "    return cvscores, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given that 50.0% of the artists are male, a random guess would have an accuracy of 50.0%.\n",
      "For the model, the mean accuracy is 69.15% and 100*STD is 0.95%\n",
      "This is a 38.30% improvement over a random guess.\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(n_estimators = 1000, \n",
    "                                   learning_rate = .05,\n",
    "                                   subsample = .5,\n",
    "                                   loss = 'deviance',\n",
    "                                   max_depth = 4,\n",
    "                                   min_samples_split = 2,\n",
    "                                   random_state = seed)\n",
    "cvscores, vals = train_validate(model, X, y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.682, 0.701]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate upper bounds on accuracy for each validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_bounds(vals, data):\n",
    "    \"\"\"Create a list of the upper bounds on accuracy for each validation set.\"\"\"\n",
    "    uppers = []\n",
    "    for val in vals:\n",
    "        data_val = data.iloc[val] \n",
    "        upper, error = genre_upperbound.UpperBound(data_val)\n",
    "        uppers.append(round(1-error,3))\n",
    "    return uppers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "uppers = upper_bounds(vals, data_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.884, 0.893, 0.88, 0.897, 0.888]\n",
      "[0.702, 0.674, 0.697, 0.716, 0.702]\n",
      "[0.182 0.219 0.183 0.181 0.186]\n"
     ]
    }
   ],
   "source": [
    "print(uppers)\n",
    "print(cvscores)\n",
    "print(np.array(uppers)-np.array(cvscores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check alignment of genders\n",
    "# n = np.random.randint(y_val0.shape[0])\n",
    "# print('y_val')\n",
    "# print(y_val0[n])\n",
    "# print('data_val')\n",
    "# print(data_val0.gender.iloc[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf-idf and then LSA in scikit learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
